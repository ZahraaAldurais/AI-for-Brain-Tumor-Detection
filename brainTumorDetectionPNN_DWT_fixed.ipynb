{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PNN for Brain Tumor Detection based on DWT"
      ],
      "metadata": {
        "id": "dyKbtl25qX3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Dataset and Libraries\n",
        "In this section we need to get the dataset. The dataset is available on a github repository. The repository contains images of brain tumor and normal brain images. The author has also performed augmentation on the dataset, and it is also provided. We use the augmented images for our training purpose.\n",
        "<br>Once the repository is cloned, we will import all the required libraries that we need for our problem."
      ],
      "metadata": {
        "id": "xla6j8sSqbph"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzFCmuPAT0jH",
        "outputId": "31ea8e2d-2e06-4cea-b590-a28b574f5d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Brain-Tumor-Detection'...\n",
            "remote: Enumerating objects: 2363, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 2363 (delta 8), reused 4 (delta 4), pack-reused 2352\u001b[K\n",
            "Receiving objects: 100% (2363/2363), 43.21 MiB | 29.82 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ],
      "source": [
        "!git  clone https://github.com/MohamedAliHabib/Brain-Tumor-Detection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "from skimage.color import rgb2gray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import matplotlib.pyplot as plt  # Import plt module for visualization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import graycoprops, graycomatrix\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "MpeF-u37jain"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing required libraries\n",
        "We need to install the libraries we are going to use for defining an example PNN for our problem."
      ],
      "metadata": {
        "id": "6A_hh7xEqftT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neupy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6utmni_pUAjR",
        "outputId": "d2bcd2ef-493a-44aa-f78b-ed7c8217b084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neupy\n",
            "  Downloading neupy-0.8.2-py2.py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from neupy) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from neupy) (1.10.1)\n",
            "INFO: pip is looking at multiple versions of neupy to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading neupy-0.8.1-py2.py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading neupy-0.8.0-py2.py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading neupy-0.7.3-py2.py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading neupy-0.7.2-py2.py3-none-any.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.5/208.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading neupy-0.7.1-py2.py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading neupy-0.7.0-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading neupy-0.6.5-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.2/197.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Theano==1.0.0 (from neupy)\n",
            "  Downloading Theano-1.0.0.tar.gz (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from neupy) (3.7.1)\n",
            "Collecting graphviz==0.5.1 (from neupy)\n",
            "  Downloading graphviz-0.5.1-py2.py3-none-any.whl (14 kB)\n",
            "Collecting tableprint==0.7.1 (from neupy)\n",
            "  Downloading tableprint-0.7.1.tar.gz (6.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting progressbar2==3.34.3 (from neupy)\n",
            "  Downloading progressbar2-3.34.3-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: python-utils>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2==3.34.3->neupy) (3.5.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tableprint==0.7.1->neupy) (1.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from tableprint==0.7.1->neupy) (0.18.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from tableprint==0.7.1->neupy) (0.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->neupy) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->neupy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->neupy) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->neupy) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->neupy) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->neupy) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->neupy) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->neupy) (2.8.2)\n",
            "Building wheels for collected packages: tableprint, Theano\n",
            "  Building wheel for tableprint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tableprint: filename=tableprint-0.7.1-py3-none-any.whl size=6164 sha256=fbc987250613421542c2e54e297bf38b3cb1a4c42a05469046c52ef02b2b79f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/f4/89/e6e577409bbc25fed6dedef9b60b60df263bd4ce1cd5c222d5\n",
            "  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Theano: filename=Theano-1.0.0-py3-none-any.whl size=2649594 sha256=18f3062485dac929fee8d41a3e32f0c47da98ee6aad07960a6b7a5010effed97\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/06/e7/b742d72dba1f1896f21519bcaf138ee5123f8e8e0cf424b382\n",
            "Successfully built tableprint Theano\n",
            "Installing collected packages: graphviz, tableprint, progressbar2, Theano, neupy\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "  Attempting uninstall: progressbar2\n",
            "    Found existing installation: progressbar2 4.2.0\n",
            "    Uninstalling progressbar2-4.2.0:\n",
            "      Successfully uninstalled progressbar2-4.2.0\n",
            "Successfully installed Theano-1.0.0 graphviz-0.5.1 neupy-0.6.5 progressbar2-3.34.3 tableprint-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neurolab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXhZ30Igezlp",
        "outputId": "1bd4f192-f261-4ad5-b636-fcc83c74f9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neurolab\n",
            "  Downloading neurolab-0.3.5.tar.gz (645 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.3/645.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: neurolab\n",
            "  Building wheel for neurolab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neurolab: filename=neurolab-0.3.5-py3-none-any.whl size=22180 sha256=7cced61e644accc5934e106af0f6f865c3e6b4d06ba079fb9f6b98fe1b9579c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/c0/44/7142fa43c89473c5e63a750a00224e5f9ec9ca80613de1f97d\n",
            "Successfully built neurolab\n",
            "Installing collected packages: neurolab\n",
            "Successfully installed neurolab-0.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declaring Variables"
      ],
      "metadata": {
        "id": "r8QSmx9Aql_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set directory path and parameters\n",
        "data_dir = '/content/Brain-Tumor-Detection/augmented data'\n",
        "categories = ['yes', 'no']  # Names of the categories\n",
        "img_size = 256  # Size of the image after resizing\n",
        "num_classes = len(categories)\n",
        "batch_size = 16\n",
        "\n"
      ],
      "metadata": {
        "id": "CuhEZHEclOt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n",
        "Here will define a function named extract_features(). this method takes image path as input and returns the features for each image. Alongwith the feature extraction, the preprocessing on the images is done. <br>\n",
        "In this method, we calculate the DWT (Discrete Wavelet Transform) features for each image so that the features maybe used for the training purposes. The DWT features are calculated based on mean, variance, maximum, minimum, energy, and entropy. We have calculated six features in this example."
      ],
      "metadata": {
        "id": "oo5w78j7qnRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pywt\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "def extract_features(image_path):\n",
        "    # Load image and resize\n",
        "    img = plt.imread(image_path)\n",
        "    img_size = 256  # Specify the desired image size\n",
        "    img = resize(img, (img_size, img_size), anti_aliasing=True)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    img_gray = rgb2gray(img)\n",
        "\n",
        "    # Calculate DWT coefficients\n",
        "    cA, (cH, cV, cD) = pywt.dwt2(img_gray, 'haar')\n",
        "\n",
        "    # Extract DWT features\n",
        "    dwt_mean = np.mean(cA)\n",
        "    dwt_var = np.var(cA)\n",
        "    dwt_max = np.max(cA)\n",
        "    dwt_min = np.min(cA)\n",
        "    dwt_energy = np.sum(np.square(cA))\n",
        "    cA_squared = np.square(cA)\n",
        "    epsilon = 1e-10  # Small constant to avoid division by zero\n",
        "    dwt_entropy = -np.sum(cA_squared * np.log(cA_squared + epsilon))\n",
        "\n",
        "    # Return features as an array\n",
        "    features = np.array([dwt_mean, dwt_var, dwt_max, dwt_min, dwt_energy, dwt_entropy])\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "B_-wOE10g3Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Features and Preparing for Training\n",
        "In this step, we load each image, and calculate the features using the method defined earlier. <br>\n",
        "Finally, the features are split in train/val categories for training and testing."
      ],
      "metadata": {
        "id": "lMAItnRUqrmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and extract features\n",
        "X = []\n",
        "y = []\n",
        "for category in categories:\n",
        "    path = os.path.join(data_dir, category)\n",
        "    for img_name in os.listdir(path):\n",
        "        img_path = os.path.join(path, img_name)\n",
        "        features = extract_features(img_path)\n",
        "        X.append(features)\n",
        "        y.append(categories.index(category))\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "VYxYsMGZlWuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition\n",
        "Here, an example PNN is defined for the explaination purposes. And, this PNN can further be improved for attaining more accuracy."
      ],
      "metadata": {
        "id": "MmF-ITh2qx_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required library for our PNN definition\n",
        "import neurolab as nl\n",
        "# Define the PNN model architecture\n",
        "input_size = X_train.shape[1]\n",
        "output_size = 1 # Binary classification task\n",
        "pnn = nl.net.newp(nl.tool.minmax(X_train), output_size)"
      ],
      "metadata": {
        "id": "oSU0j_n6jkcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "In this step, we train the model. We run the training for 10 epochs total. The hyperparameters (batch_size and epochs) can be changed for attaining better results."
      ],
      "metadata": {
        "id": "JWT8xX5lrPHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the PNN model\n",
        "pnn.train(X_train, y_train.reshape(-1, output_size), epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGY0gPC1rGti",
        "outputId": "17a0843f-2fb8-4bf1-e359-084742b90ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum number of train epochs is reached\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[390.5,\n",
              " 338.5,\n",
              " 355.5,\n",
              " 435.5,\n",
              " 353.0,\n",
              " 386.5,\n",
              " 335.5,\n",
              " 435.5,\n",
              " 406.0,\n",
              " 332.0,\n",
              " 332.0,\n",
              " 331.5,\n",
              " 301.0,\n",
              " 418.5,\n",
              " 308.0,\n",
              " 363.5,\n",
              " 406.5,\n",
              " 335.5,\n",
              " 357.0,\n",
              " 333.0,\n",
              " 373.0,\n",
              " 331.0,\n",
              " 435.5,\n",
              " 405.0,\n",
              " 338.0,\n",
              " 405.5,\n",
              " 385.5,\n",
              " 332.0,\n",
              " 435.5,\n",
              " 349.5,\n",
              " 363.5,\n",
              " 307.5,\n",
              " 361.0,\n",
              " 383.0,\n",
              " 376.0,\n",
              " 353.5,\n",
              " 386.5,\n",
              " 406.5,\n",
              " 299.5,\n",
              " 309.5,\n",
              " 435.5,\n",
              " 307.0,\n",
              " 440.5,\n",
              " 323.0,\n",
              " 435.5,\n",
              " 370.0,\n",
              " 332.5,\n",
              " 355.0,\n",
              " 435.5,\n",
              " 356.0]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Testing\n",
        "Now the accuracy and loss is calculated for our model. Here, val dataset is used, we can devide our dataset into three categories, train, test, and val as well. "
      ],
      "metadata": {
        "id": "v5hqGPxgrTBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the PNN model on the test data\n",
        "predictions = pnn.sim(X_val)\n",
        "accuracy = (predictions == y_val.reshape(-1, output_size)).mean()\n",
        "print(f'Test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlzhkCVLrKnB",
        "outputId": "26e5af64-8bfa-412a-c0ce-062f5fff87c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.5786924939467313\n"
          ]
        }
      ]
    }
  ]
}